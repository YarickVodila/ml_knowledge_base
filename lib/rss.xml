<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[База знаний по ML]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>База знаний по ML</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Sun, 16 Feb 2025 16:35:18 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Sun, 16 Feb 2025 16:35:18 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Вероятность]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="#Maximum likelihood estimation" href="about:blank#Maximum_likelihood_estimation" class="internal-link" target="_self" rel="noopener nofollow">Maximum likelihood estimation</a>
<br><br>Maximum likelihood estimation (Оценка максимального правдоподобия) - это метод оценивания неизвестных параметров путём максимизации функции правдоподобия. Данный метод основывается на предположении, что вся информация о статистической выборке содержится в функции правдоподобия.<br>Функция правдоподобия  представляет собой вероятность получения наблюдаемых данных  и  при заданном значении параметра .  Задача метода максимального правдоподобия состоит в том, чтобы найти такое значение параметра , которое максимизирует эту вероятность. Другими словами, мы ищем такое значение , при котором наблюдаемые данные и  были бы наиболее вероятными.<br>В частности, пример  показывает, что целью является поиск такого значения , которое максимизирует функцию правдоподобия ]]></description><link>Base math for ML\Вероятность.html</link><guid isPermaLink="false">Base math for ML/Вероятность.md</guid><pubDate>Sun, 16 Feb 2025 14:25:37 GMT</pubDate></item><item><title><![CDATA[Дикая база по основам работы с текстами]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="#Стемминг и Леммантизация" href="about:blank#Стемминг_и_Леммантизация" class="internal-link" target="_self" rel="noopener nofollow">Стемминг и Леммантизация</a>
<br><a data-href="#Bag of words" href="about:blank#Bag_of_words" class="internal-link" target="_self" rel="noopener nofollow">Bag of words</a>
<br><a data-href="#TF-IDF" href="about:blank#TF-IDF" class="internal-link" target="_self" rel="noopener nofollow">TF-IDF</a>
<br><a data-href="#Word2Vec" href="about:blank#Word2Vec" class="internal-link" target="_self" rel="noopener nofollow">Word2Vec</a>
<br><a data-href="#Токенизация" href="about:blank#Токенизация" class="internal-link" target="_self" rel="noopener nofollow">Токенизация</a>
<br><br>Стемминг - &nbsp;это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы. Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа. <br>Пример
Airliner -&gt; Airlin
<br>Лемматизация&nbsp;— это процесс приведения слова к его базовой форме (лемме) путём удаления суффиксов и преобразования слова в его нормальную форму. Например, лемматизация слова «бежал» даст базовую форму «бежать».<br>Пример
бежал -&gt; бежать
<br><br>Bag of Words (BoW) — это метод представления текстовых данных в виде числовых векторов, который используется в задачах обработки естественного языка (NLP). Основная идея заключается в том, что текст рассматривается как неупорядоченный набор слов (или токенов), а порядок слов игнорируется. Каждое слово становится признаком, а значение признака — это частота его появления в тексте.<br><br>Предположим, у нас есть три текстовых документа:<br>
<br>Документ 1: "Я люблю машинное обучение"
<br>Документ 2: "Машинное обучение это интересно"
<br>Документ 3: "Я люблю программирование"
<br><br>Сначала из всех документов выделяем уникальные слова (токены):<br>
<br>я
<br>люблю
<br>машинное
<br>обучение
<br>это
<br>интересно
<br>программирование
<br><br>Теперь для каждого документа создаем вектор, где каждый элемент соответствует частоте появления слова из словаря. Если слово отсутствует в документе, ставим 0.<br><br><br>
<br>Документ 1: [1, 1, 1, 1, 0, 0, 0]
<br>Документ 2: [0, 0, 1, 1, 1, 1, 0]
<br>Документ 3: [1, 1, 0, 0, 0, 0, 1]
<br>Таким образом, каждый документ представлен в виде числового вектора, который можно использовать для дальнейшего анализа, например, в задачах классификации или кластеризации текстов.<br><br>TF-IDF (Term Frequency-Inverse Document Frequency) — это метод представления текстовых данных в виде числовых векторов, который учитывает не только частоту слова в документе, но и его важность в коллекции документов. TF-IDF состоит из двух компонентов:<br>
<br>TF (Term Frequency) — частота термина в документе. Показывает, насколько часто слово встречается в конкретном документе.
<br>IDF (Inverse Document Frequency) — обратная частота документа. Показывает, насколько редким является слово во всей коллекции документов. Чем реже слово встречается в документах, тем выше его вес.
<br>Формула TF-IDF:<br><br><br>Предположим, у нас есть три текстовых документа:<br>
<br>Документ 1: "Я люблю машинное обучение"
<br>Документ 2: "Машинное обучение это интересно"
<br>Документ 3: "Я люблю программирование"
<br><br>Из всех документов выделяем уникальные слова (токены):<br>
<br>я
<br>люблю
<br>машинное
<br>обучение
<br>это
<br>интересно
<br>программирование
<br><br>Для каждого документа вычисляем частоту каждого слова (TF). Например, для Документа 1:<br>
<br>"я" встречается 1 раз, всего слов в документе 4, поэтому TF = 1/4 = 0.25.
<br><br>IDF вычисляется по формуле:<br><br>где:<br>
<br>( N ) — общее количество документов (в нашем случае 3),
<br>( n ) — количество документов, содержащих данное слово.
<br>Например, для слова "машинное":<br>
<br>Оно встречается в 2 документах (Документ 1 и Документ 2), поэтому IDF = log(3/2) ≈ 0.176.
<br><br>Умножаем TF на IDF для каждого слова в каждом документе.<br><br><br><br>
<br>Документ 1: [0.044, 0.044, 0.044, 0.044, 0, 0, 0]
<br>Документ 2: [0, 0, 0.044, 0.044, 0.119, 0.119, 0]
<br>Документ 3: [0.044, 0.044, 0, 0, 0, 0, 0.119]
<br><br>Word2Vec — это популярная модель для создания векторных представлений слов (word embeddings). Она преобразует слова в числовые векторы фиксированной длины, которые сохраняют семантические и синтаксические отношения между словами. Эти векторы могут использоваться в различных задачах обработки естественного языка (NLP), таких как классификация текста, машинный перевод, анализ тональности и другие.<br><br>Модели Word2Vec обучаются на больших текстовых корпусах с использованием нейронных сетей. Основная идея заключается в том, что слова, которые встречаются в похожих контекстах, имеют схожие значения. Обучение происходит двумя основными способами:<br>
<br>
Skip-Gram: Модель предсказывает контекстные слова (окружение) по заданному целевому слову. Например, для слова "кошка" модель пытается предсказать такие слова, как "мяукает", "пушистая" и т.д.

<br>
CBOW (Continuous Bag of Words): Модель предсказывает целевое слово на основе его контекста. Например, по словам "мяукает", "пушистая" модель пытается предсказать слово "кошка".

<br>Оба подхода используют нейронную сеть с одним скрытым слоем, которая обучается минимизировать ошибку предсказания.<br><br>На вход модели Word2Vec подаётся текстовый корпус, который предварительно обрабатывается. Текст кодируется следующим образом:<br>
<br>Токенизация: Текст разбивается на отдельные слова или токены.
<br>Создание словаря: Уникальные слова из текста собираются в словарь, где каждому слову присваивается уникальный индекс.
<br>One-Hot Encoding: Каждое слово представляется в виде one-hot вектора — это вектор, где все элементы равны нулю, кроме одного, который соответствует индексу слова в словаре.
<br>Контекстные пары: Для обучения модели формируются пары "целевое слово — контекстное слово". Например, для предложения "кошка мяукает" и окна контекста размером 1, будут созданы пары: ("кошка", "мяукает") и ("мяукает", "кошка").
<br>После обучения модели, каждое слово представляется в виде плотного вектора (embedding), который захватывает семантические и синтаксические свойства слова. Эти векторы могут быть использованы для различных задач NLP.<br><br><br>Byte-Pair Encoding (BPE) — это метод токенизации, который используется для разбиения текста на подсловные единицы (субтокены). Он особенно полезен для обработки текста в моделях машинного обучения, таких как нейронные сети, так как позволяет эффективно работать с редкими словами и уменьшает размер словаря.<br><br>
<br>
Инициализация:

<br>Начните с разбиения текста на символы (или байты). Например, слово "hello" будет разбито на ['h', 'e', 'l', 'l', 'o'].
<br>Создайте начальный словарь, состоящий из всех уникальных символов в тексте.


<br>
Подсчет частот пар:

<br>Подсчитайте частоту всех возможных пар символов (или байтов) в тексте. Например, для слова "hello" пары будут ['he', 'el', 'll', 'lo'].


<br>
Слияние наиболее частых пар:

<br>Найдите пару символов, которая встречается чаще всего, и объедините её в новый символ (токен). Например, если пара 'l', 'l' встречается чаще всего, то она будет заменена на новый токен 'll'.
<br>Добавьте новый токен в словарь.


<br>
Повторение:

<br>Повторяйте шаги 2 и 3 до тех пор, пока не будет достигнуто желаемое количество токенов в словаре или пока не будет выполнено определённое количество итераций.


<br>
Токенизация:

<br>После обучения BPE, текст можно токенизировать, разбивая его на подсловные единицы, используя обученный словарь.


<br><br>
<br>
Подготовка данных:

<br>Соберите большой корпус текста, на котором будет обучаться BPE. Это может быть текст из книг, статей, или любого другого источника.


<br>
Инициализация словаря:

<br>Начните с создания начального словаря, состоящего из всех уникальных символов в корпусе.


<br>
Итеративное слияние:

<br>На каждой итерации подсчитывайте частоту всех пар символов и сливайте наиболее частую пару в новый токен. Повторяйте этот процесс до достижения желаемого размера словаря.


<br>
Сохранение словаря:

<br>После завершения обучения сохраните словарь BPE, который будет использоваться для токенизации новых текстов.


<br><br>WordPiece — это алгоритм токенизации, используемый для разбиения текста на субтокены (подслова). Он широко применяется в моделях обработки естественного языка (NLP), таких как BERT, DistilBERT и других трансформерных архитектурах. WordPiece позволяет эффективно обрабатывать редкие слова и сокращать размер словаря, что делает его популярным выбором для задач NLP.<br><br>
<br>
Инициализация словаря:

<br>Начинается с базового словаря, который может включать отдельные символы, часто встречающиеся слова и специальные токены (например, [CLS], [SEP]).


<br>
Разбиение текста на токены:

<br>Исходный текст сначала разбивается на слова (например, по пробелам).
<br>Каждое слово далее разбивается на символы или последовательности символов, которые уже есть в словаре.


<br>
Поиск оптимального разбиения:

<br>Для каждого слова алгоритм ищет наиболее вероятное разбиение на субтокены, используя текущий словарь.
<br>Разбиение выбирается таким образом, чтобы максимизировать вероятность последовательности субтокенов.


<br>
Обучение словаря:

<br>На основе частотности встречаемости последовательностей символов в обучающем корпусе.
<br>На каждом шаге добавляются новые субтокены, которые наиболее часто встречаются в тексте.


<br><br>
<br>
Подготовка данных:

<br>Соберите большой корпус текстов, на котором будет обучаться модель.
<br>Очистите текст (удалите ненужные символы, приведите к нижнему регистру и т.д.).


<br>
Инициализация словаря:

<br>Начните с базового словаря, включающего символы, часто встречающиеся слова и специальные токены.


<br>
Итеративное обучение:

<br>На каждом шаге анализируйте текст и находите наиболее частые пары символов или субтокенов.
<br>Объединяйте эти пары в новые субтокены и добавляйте их в словарь.
<br>Повторяйте процесс до достижения желаемого размера словаря или пока не перестанете находить полезные пары.


<br>
Оценка и настройка:

<br>После обучения оцените качество токенизации на тестовом наборе данных.
<br>При необходимости настройте параметры обучения (например, размер словаря, частотные пороги).


<br><br>WordPiece используется в различных современных моделях NLP, включая:<br>
<br>BERT (Bidirectional Encoder Representations from Transformers) — одна из первых моделей, использующих WordPiece.
<br>DistilBERT — облегченная версия BERT.
<br>ALBERT (A Lite BERT) — оптимизированная версия BERT с уменьшенным количеством параметров.
<br>Electra — модель, которая использует WordPiece для токенизации входных данных.
<br><br>
<br>Эффективная обработка редких слов.
<br>Сокращение размера словаря.
<br>Устойчивость к опечаткам и редким терминам.
]]></description><link>NLP\Дикая база по основам работы с текстами.html</link><guid isPermaLink="false">NLP/Дикая база по основам работы с текстами.md</guid><pubDate>Sat, 15 Feb 2025 12:36:10 GMT</pubDate></item><item><title><![CDATA[ML_knowledge_base]]></title><description><![CDATA[ 
 <br><br><br>
<br>Bias and Varience
<br>Статистика
<br><a data-href="Вероятность" href="Base math for ML\Вероятность.html" class="internal-link" target="_self" rel="noopener nofollow">Вероятность</a>
<br><br>
<br>Линейные модели
<br>Деревья
<br>Preprocessing данных
<br><br>
<br>Структура нейросетей
<br>Оптимизаторы
<br>Слои нормализации
<br><br>
<br><a data-href="Дикая база по основам работы с текстами" href="NLP\Дикая база по основам работы с текстами.html" class="internal-link" target="_self" rel="noopener nofollow">Дикая база по основам работы с текстами</a>
<br>Transformers
<br>RAG
<br>print("Hello world and go!")
]]></description><link>ML_knowledge_base.html</link><guid isPermaLink="false">ML_knowledge_base.md</guid><pubDate>Sun, 16 Feb 2025 12:38:07 GMT</pubDate></item></channel></rss>
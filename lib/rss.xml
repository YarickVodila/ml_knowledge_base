<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[База знаний по ML]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>База знаний по ML</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 12 Feb 2025 15:24:32 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 12 Feb 2025 15:24:31 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[Дикая база по основам работы с текстами]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="#Стемминг и Леммантизация" href="about:blank#Стемминг_и_Леммантизация" class="internal-link" target="_self" rel="noopener nofollow">Стемминг и Леммантизация</a>
<br><a data-href="#Bag of words" href="about:blank#Bag_of_words" class="internal-link" target="_self" rel="noopener nofollow">Bag of words</a>
<br><a data-href="#TF-IDF" href="about:blank#TF-IDF" class="internal-link" target="_self" rel="noopener nofollow">TF-IDF</a>
<br><a data-href="#Word2Vec" href="about:blank#Word2Vec" class="internal-link" target="_self" rel="noopener nofollow">Word2Vec</a>
<br><a data-href="#Токенизация" href="about:blank#Токенизация" class="internal-link" target="_self" rel="noopener nofollow">Токенизация</a>
<br><br>Стемминг - &nbsp;это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы. Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа. <br>Пример
Airliner -&gt; Airlin
<br>Лемматизация&nbsp;— это процесс приведения слова к его базовой форме (лемме) путём удаления суффиксов и преобразования слова в его нормальную форму. Например, лемматизация слова «бежал» даст базовую форму «бежать».<br>Пример
бежал -&gt; бежать
<br><br>Bag of Words (BoW) — это метод представления текстовых данных в виде числовых векторов, который используется в задачах обработки естественного языка (NLP). Основная идея заключается в том, что текст рассматривается как неупорядоченный набор слов (или токенов), а порядок слов игнорируется. Каждое слово становится признаком, а значение признака — это частота его появления в тексте.<br><br>Предположим, у нас есть три текстовых документа:<br>
<br>Документ 1: "Я люблю машинное обучение"
<br>Документ 2: "Машинное обучение это интересно"
<br>Документ 3: "Я люблю программирование"
<br><br>Сначала из всех документов выделяем уникальные слова (токены):<br>
<br>я
<br>люблю
<br>машинное
<br>обучение
<br>это
<br>интересно
<br>программирование
<br><br>Теперь для каждого документа создаем вектор, где каждый элемент соответствует частоте появления слова из словаря. Если слово отсутствует в документе, ставим 0.<br><br><br>
<br>Документ 1: [1, 1, 1, 1, 0, 0, 0]
<br>Документ 2: [0, 0, 1, 1, 1, 1, 0]
<br>Документ 3: [1, 1, 0, 0, 0, 0, 1]
<br>Таким образом, каждый документ представлен в виде числового вектора, который можно использовать для дальнейшего анализа, например, в задачах классификации или кластеризации текстов.<br><br>TF-IDF (Term Frequency-Inverse Document Frequency) — это метод представления текстовых данных в виде числовых векторов, который учитывает не только частоту слова в документе, но и его важность в коллекции документов. TF-IDF состоит из двух компонентов:<br>
<br>TF (Term Frequency) — частота термина в документе. Показывает, насколько часто слово встречается в конкретном документе.
<br>IDF (Inverse Document Frequency) — обратная частота документа. Показывает, насколько редким является слово во всей коллекции документов. Чем реже слово встречается в документах, тем выше его вес.
<br>Формула TF-IDF:<br><br><br>Предположим, у нас есть три текстовых документа:<br>
<br>Документ 1: "Я люблю машинное обучение"
<br>Документ 2: "Машинное обучение это интересно"
<br>Документ 3: "Я люблю программирование"
<br><br>Из всех документов выделяем уникальные слова (токены):<br>
<br>я
<br>люблю
<br>машинное
<br>обучение
<br>это
<br>интересно
<br>программирование
<br><br>Для каждого документа вычисляем частоту каждого слова (TF). Например, для Документа 1:<br>
<br>"я" встречается 1 раз, всего слов в документе 4, поэтому TF = 1/4 = 0.25.
<br><br>IDF вычисляется по формуле:<br><br>где:<br>
<br>( N ) — общее количество документов (в нашем случае 3),
<br>( n ) — количество документов, содержащих данное слово.
<br>Например, для слова "машинное":<br>
<br>Оно встречается в 2 документах (Документ 1 и Документ 2), поэтому IDF = log(3/2) ≈ 0.176.
<br><br>Умножаем TF на IDF для каждого слова в каждом документе.<br><br><br><br>
<br>Документ 1: [0.044, 0.044, 0.044, 0.044, 0, 0, 0]
<br>Документ 2: [0, 0, 0.044, 0.044, 0.119, 0.119, 0]
<br>Документ 3: [0.044, 0.044, 0, 0, 0, 0, 0.119]
<br><br>Word2Vec — это популярная модель для создания векторных представлений слов (word embeddings). Она преобразует слова в числовые векторы фиксированной длины, которые сохраняют семантические и синтаксические отношения между словами. Эти векторы могут использоваться в различных задачах обработки естественного языка (NLP), таких как классификация текста, машинный перевод, анализ тональности и другие.<br><br>Модели Word2Vec обучаются на больших текстовых корпусах с использованием нейронных сетей. Основная идея заключается в том, что слова, которые встречаются в похожих контекстах, имеют схожие значения. Обучение происходит двумя основными способами:<br>
<br>
Skip-Gram: Модель предсказывает контекстные слова (окружение) по заданному целевому слову. Например, для слова "кошка" модель пытается предсказать такие слова, как "мяукает", "пушистая" и т.д.

<br>
CBOW (Continuous Bag of Words): Модель предсказывает целевое слово на основе его контекста. Например, по словам "мяукает", "пушистая" модель пытается предсказать слово "кошка".

<br>Оба подхода используют нейронную сеть с одним скрытым слоем, которая обучается минимизировать ошибку предсказания.<br><br>На вход модели Word2Vec подаётся текстовый корпус, который предварительно обрабатывается. Текст кодируется следующим образом:<br>
<br>Токенизация: Текст разбивается на отдельные слова или токены.
<br>Создание словаря: Уникальные слова из текста собираются в словарь, где каждому слову присваивается уникальный индекс.
<br>One-Hot Encoding: Каждое слово представляется в виде one-hot вектора — это вектор, где все элементы равны нулю, кроме одного, который соответствует индексу слова в словаре.
<br>Контекстные пары: Для обучения модели формируются пары "целевое слово — контекстное слово". Например, для предложения "кошка мяукает" и окна контекста размером 1, будут созданы пары: ("кошка", "мяукает") и ("мяукает", "кошка").
<br>После обучения модели, каждое слово представляется в виде плотного вектора (embedding), который захватывает семантические и синтаксические свойства слова. Эти векторы могут быть использованы для различных задач NLP.<br><br><br>Byte-Pair Encoding (BPE) — это метод токенизации, который используется для разбиения текста на подсловные единицы (субтокены). Он особенно полезен для обработки текста в моделях машинного обучения, таких как нейронные сети, так как позволяет эффективно работать с редкими словами и уменьшает размер словаря.<br><br>
<br>Инициализация:
<br>
<br>Начните с разбиения текста на символы (или байты). Например, слово "hello" будет разбито на ['h', 'e', 'l', 'l', 'o'].
<br>Создайте начальный словарь, состоящий из всех уникальных символов в тексте.
<br>
<br>Подсчет частот пар:
<br>
<br>Подсчитайте частоту всех возможных пар символов (или байтов) в тексте. Например, для слова "hello" пары будут ['he', 'el', 'll', 'lo'].
<br>
<br>Слияние наиболее частых пар:
<br>
<br>Найдите пару символов, которая встречается чаще всего, и объедините её в новый символ (токен). Например, если пара 'l', 'l' встречается чаще всего, то она будет заменена на новый токен 'll'.
<br>Добавьте новый токен в словарь.
<br>
<br>Повторение:
<br>
<br>Повторяйте шаги 2 и 3 до тех пор, пока не будет достигнуто желаемое количество токенов в словаре или пока не будет выполнено определённое количество итераций.
<br>
<br>Токенизация:
<br>
<br>После обучения BPE, текст можно токенизировать, разбивая его на подсловные единицы, используя обученный словарь.
<br><br>
<br>Подготовка данных:
<br>
<br>Соберите большой корпус текста, на котором будет обучаться BPE. Это может быть текст из книг, статей, или любого другого источника.
<br>
<br>Инициализация словаря:
<br>
<br>Начните с создания начального словаря, состоящего из всех уникальных символов в корпусе.
<br>
<br>Итеративное слияние:
<br>
<br>На каждой итерации подсчитывайте частоту всех пар символов и сливайте наиболее частую пару в новый токен. Повторяйте этот процесс до достижения желаемого размера словаря.
<br>
<br>Сохранение словаря:
<br>
<br>После завершения обучения сохраните словарь BPE, который будет использоваться для токенизации новых текстов.
<br><br>BPE широко используется в современных моделях обработки естественного языка (NLP), таких как:<br>
<br>Transformer-based модели: BPE используется в моделях, таких как GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), и других.
<br>Нейронные машинные переводы: BPE помогает уменьшить размер словаря и улучшить качество перевода, особенно для редких слов.
<br>Генерация текста: BPE позволяет моделям генерировать текст на уровне подслов, что улучшает качество генерации.
<br><br>WordPiece — это алгоритм токенизации, используемый для разбиения текста на субтокены (подслова). Он широко применяется в моделях обработки естественного языка (NLP), таких как BERT, DistilBERT и других трансформерных архитектурах. WordPiece позволяет эффективно обрабатывать редкие слова и сокращать размер словаря, что делает его популярным выбором для задач NLP.<br><br>
<br>Инициализация словаря:
<br>
<br>Начинается с базового словаря, который может включать отдельные символы, часто встречающиеся слова и специальные токены (например, [CLS], [SEP]).
<br>
<br>Разбиение текста на токены:
<br>
<br>Исходный текст сначала разбивается на слова (например, по пробелам).
<br>Каждое слово далее разбивается на символы или последовательности символов, которые уже есть в словаре.
<br>
<br>Поиск оптимального разбиения:
<br>
<br>Для каждого слова алгоритм ищет наиболее вероятное разбиение на субтокены, используя текущий словарь.
<br>Разбиение выбирается таким образом, чтобы максимизировать вероятность последовательности субтокенов.
<br>
<br>Обучение словаря:
<br>
<br>На основе частотности встречаемости последовательностей символов в обучающем корпусе.
<br>На каждом шаге добавляются новые субтокены, которые наиболее часто встречаются в тексте.
<br><br>
<br>Подготовка данных:
<br>
<br>Соберите большой корпус текстов, на котором будет обучаться модель.
<br>Очистите текст (удалите ненужные символы, приведите к нижнему регистру и т.д.).
<br>
<br>Инициализация словаря:
<br>
<br>Начните с базового словаря, включающего символы, часто встречающиеся слова и специальные токены.
<br>
<br>Итеративное обучение:
<br>
<br>На каждом шаге анализируйте текст и находите наиболее частые пары символов или субтокенов.
<br>Объединяйте эти пары в новые субтокены и добавляйте их в словарь.
<br>Повторяйте процесс до достижения желаемого размера словаря или пока не перестанете находить полезные пары.
<br>
<br>Оценка и настройка:
<br>
<br>После обучения оцените качество токенизации на тестовом наборе данных.
<br>При необходимости настройте параметры обучения (например, размер словаря, частотные пороги).
<br><br>WordPiece используется в различных современных моделях NLP, включая:<br>
<br>BERT (Bidirectional Encoder Representations from Transformers) — одна из первых моделей, использующих WordPiece.
<br>DistilBERT — облегченная версия BERT.
<br>ALBERT (A Lite BERT) — оптимизированная версия BERT с уменьшенным количеством параметров.
<br>Electra — модель, которая использует WordPiece для токенизации входных данных.
<br><br>
<br>Эффективная обработка редких слов.
<br>Сокращение размера словаря.
<br>Устойчивость к опечаткам и редким терминам.
]]></description><link>NLP\Дикая база по основам работы с текстами.html</link><guid isPermaLink="false">NLP/Дикая база по основам работы с текстами.md</guid><pubDate>Wed, 12 Feb 2025 14:25:01 GMT</pubDate></item><item><title><![CDATA[База знаний по ML]]></title><description><![CDATA[ 
 <br><br>
<br><a data-href="Дикая база по основам работы с текстами" href="NLP\Дикая база по основам работы с текстами.html" class="internal-link" target="_self" rel="noopener nofollow">Дикая база по основам работы с текстами</a>
<br>Transformers
<br>RAG
<br><br>Hello world
]]></description><link>База знаний по ML.html</link><guid isPermaLink="false">База знаний по ML.md</guid><pubDate>Wed, 12 Feb 2025 15:17:08 GMT</pubDate></item></channel></rss>